{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPp4kT1mOT6GJXqW9RW3YVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CT2020Hypernym/hyponym_and_hyperonym/blob/CNN_Maria/Maria/CNN_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsdgWYM0keVL",
        "colab_type": "code",
        "outputId": "31f87e01-6bb2-4cc8-c82a-8fc7805f8432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install spacy==2.2.3\n",
        "!pip install pymorphy2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.17.5)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (1.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (0.6.0)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.3MB/s \n",
            "\u001b[?25hCollecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 55.6MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (0.9.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (42.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3) (2.21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3) (8.0.2)\n",
            "Installing collected packages: blis, preshed, thinc, catalogue, spacy\n",
            "  Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.9\n",
            "    Uninstalling spacy-2.1.9:\n",
            "      Successfully uninstalled spacy-2.1.9\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "preshed",
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XYP3Jmhl1FN",
        "colab_type": "code",
        "outputId": "67bffdc6-6844-4cab-e806-8dbc9cf0a3d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/buriy/spacy-ru.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spacy-ru'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 313 (delta 4), reused 7 (delta 2), pack-reused 301\u001b[K\n",
            "Receiving objects: 100% (313/313), 117.34 MiB | 11.80 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLcGO28QmBuB",
        "colab_type": "code",
        "outputId": "d83b5e10-5173-45f3-8b84-b4bccfbd0bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "import pandas as pd\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "from spacy.lang.ru import Russian\n",
        "\n",
        "n_texts = 7915\n",
        "n_iter = 10\n",
        "\n",
        "def dataParser():\n",
        "  #maybe create new file and work with it\n",
        "  data = pd.read_csv('./hyponym_and_hyperonym/DataSet/pos_pairs.csv')\n",
        "  data['concat'] = data['child_context'] + ' ' + data['parent_context']\n",
        "  train_pos_df = data[['concat', 'label']].dropna()\n",
        "  data0 = pd.read_csv('./hyponym_and_hyperonym/DataSet/neg_pairs.csv')\n",
        "  data0['concat'] = data0['child_context'] + ' ' + data0['parent_context']\n",
        "  train_neg_df = data0[['concat', 'label']].dropna()\n",
        "  train_df=train_pos_df.append(train_neg_df)\n",
        "\n",
        "  train_df['tuples'] = train_df.apply(\n",
        "      lambda row: (row['concat'],row['label']), axis=1)\n",
        "  train = train_df['tuples'].tolist()\n",
        "  return train\n",
        "\n",
        "def load_data(limit=0, split=0.8):\n",
        "    train_data = dataParser()\n",
        "    random.shuffle(train_data)\n",
        "    train_data = train_data[-limit:]\n",
        "    texts, labels = zip(*train_data)\n",
        "    cats = [{'1': bool(int(y)), '0': not bool(int(y))} for y in labels]\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 0.0  # True positives\n",
        "    fp = 1e-8  # False positives\n",
        "    fn = 1e-8  # False negatives\n",
        "    tn = 0.0  # True negatives\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if label == '0':\n",
        "                continue\n",
        "            mean = 0.5\n",
        "            if score >= mean and gold[label] >= mean:\n",
        "                tp += 1.0\n",
        "            elif score >= mean and gold[label] < mean:\n",
        "                fp += 1.0\n",
        "            elif score < mean and gold[label] < mean:\n",
        "                tn += 1\n",
        "            elif score < mean and gold[label] >= mean:\n",
        "                fn += 1\n",
        "    print(tp, fp, tn, fn, tp + fp + tn + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    if (precision + recall) == 0:\n",
        "        f_score = 0.0\n",
        "    else:\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #load model (ru)\n",
        "    #nlp = spacy.load('spacy-ru/ru2')\n",
        "    nlp = Russian()\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
        "\n",
        "    if 'textcat' not in nlp.pipe_names:\n",
        "      textcat = nlp.create_pipe('textcat')\n",
        "      nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "      textcat = nlp.get_pipe('textcat')\n",
        "\n",
        "    #categories\n",
        "    textcat.add_label(\"1\") #with hypernymy\n",
        "    textcat.add_label(\"0\") #without\n",
        "\n",
        "    #load dataset\n",
        "\n",
        "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data()\n",
        "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
        "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
        "      .format(n_texts, len(train_texts), len(dev_texts)))\n",
        "\n",
        "    #train model\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        print(\"Training the model...\")\n",
        "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
        "                          losses=losses)\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                # evaluate on the dev data split off in load_data()\n",
        "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
        "            print(\n",
        "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                    losses[\"textcat\"],\n",
        "                    scores[\"textcat_p\"],\n",
        "                    scores[\"textcat_r\"],\n",
        "                    scores[\"textcat_f\"],\n",
        "                )\n",
        "            )\n",
        "    print(\"Training the model ended.\")\n",
        "\n",
        "    #save the model\n",
        "    output_dir = %pwd\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 7915 examples (6332 training, 1583 evaluation)\n",
            "Training the model...\n",
            "LOSS \t  P  \t  R  \t  F  \n",
            "888.0 60.00000001 602.0 33.00000001 1583.0000000199998\n",
            "11.067\t0.937\t0.964\t0.950\n",
            "888.0 28.00000001 634.0 33.00000001 1583.0000000199998\n",
            "3.404\t0.969\t0.964\t0.967\n",
            "887.0 29.00000001 633.0 34.00000001 1583.0000000199998\n",
            "2.238\t0.968\t0.963\t0.966\n",
            "883.0 27.00000001 635.0 38.00000001 1583.0000000199998\n",
            "1.687\t0.970\t0.959\t0.965\n",
            "880.0 22.00000001 640.0 41.00000001 1583.0000000199998\n",
            "1.257\t0.976\t0.955\t0.965\n",
            "888.0 21.00000001 641.0 33.00000001 1583.0000000199998\n",
            "1.004\t0.977\t0.964\t0.970\n",
            "890.0 14.00000001 648.0 31.00000001 1583.0000000199998\n",
            "0.855\t0.985\t0.966\t0.975\n",
            "893.0 17.00000001 645.0 28.00000001 1583.0000000199998\n",
            "0.764\t0.981\t0.970\t0.975\n",
            "895.0 18.00000001 644.0 26.00000001 1583.0000000199998\n",
            "0.520\t0.980\t0.972\t0.976\n",
            "895.0 19.00000001 643.0 26.00000001 1583.0000000199998\n",
            "0.515\t0.979\t0.972\t0.975\n",
            "Training the model ended.\n",
            "Saved model to /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijH3YHa5kS7",
        "colab_type": "code",
        "outputId": "b1b8828e-785c-4271-c059-7c558ccd7553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "!pip install git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
        "!git clone https://github.com/CT2020Hypernym/hyponym_and_hyperonym.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
            "  Cloning https://github.com/aatimofeev/spacy_russian_tokenizer.git to /tmp/pip-req-build-e3mw6_iw\n",
            "  Running command git clone -q https://github.com/aatimofeev/spacy_russian_tokenizer.git /tmp/pip-req-build-e3mw6_iw\n",
            "Requirement already satisfied (use --upgrade to upgrade): spacy-russian-tokenizer==0.1.1 from git+https://github.com/aatimofeev/spacy_russian_tokenizer.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from spacy-russian-tokenizer==0.1.1) (2.2.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.17.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (42.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.1)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (7.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy->spacy-russian-tokenizer==0.1.1) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2019.11.28)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (8.0.2)\n",
            "Building wheels for collected packages: spacy-russian-tokenizer\n",
            "  Building wheel for spacy-russian-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-russian-tokenizer: filename=spacy_russian_tokenizer-0.1.1-cp36-none-any.whl size=12674 sha256=ff4bdb3ac516493ef474cb5e3c87ccb9b2f8b1be6820da20d151043bd9a6c887\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dxzecbu4/wheels/37/3b/bb/cfe712f7c0b78cd08f4a2ef122d17748baf9d4bebecf2e5a54\n",
            "Successfully built spacy-russian-tokenizer\n",
            "Cloning into 'hyponym_and_hyperonym'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 163 (delta 12), reused 4 (delta 1), pack-reused 124\u001b[K\n",
            "Receiving objects: 100% (163/163), 8.24 MiB | 22.99 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSVhOwSlLi4t",
        "colab_type": "code",
        "outputId": "5c71189b-baea-4e97-d64a-ff396ebbfdf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/dialogue-evaluation/taxonomy-enrichment.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'taxonomy-enrichment'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 114 (delta 16), reused 33 (delta 6), pack-reused 63\u001b[K\n",
            "Receiving objects: 100% (114/114), 17.72 MiB | 8.04 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05930542-99ee-4f02-a55a-18361fa4b96c",
        "id": "aSGDVvkZ4Mw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"/content\")\n",
        "\n",
        "doc = nlp('все растения имеют здоровый и ухоженный вид. Это внешняя черта, но она говорит.')\n",
        "print(doc.cats)\n",
        "doc = nlp('их колониального правления, это их правовая система, основанная на общем праве ,28 сентября 1998 г. за нарушение правил торговли должностное лицо было')\n",
        "print(doc.cats)\n",
        "\n",
        "doc = nlp('утка взлетела в небо. водоплавающая птица расправила крылья')\n",
        "print(doc.cats)\n",
        "doc = nlp('утка взлетела в небо. Это внешняя черта, но она говорит.')\n",
        "print(doc.cats)\n",
        "\n",
        "doc = nlp('все растения имеют здоровый и ухоженный вид. 28 сентября 1998 г. за нарушение правил торговли должностное лицо было')\n",
        "print(doc.cats)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1': 0.9572031497955322, '0': 0.024976668879389763}\n",
            "{'1': 0.0029625261668115854, '0': 0.9968359470367432}\n",
            "{'1': 0.9911258816719055, '0': 0.006621245760470629}\n",
            "{'1': 0.9786659479141235, '0': 0.014566497877240181}\n",
            "{'1': 0.9118579626083374, '0': 0.057489894330501556}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
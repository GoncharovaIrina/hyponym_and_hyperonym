{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVdeedoZbbLt+vSaZTOPw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoncharovaIrina/hyponym_and_hyperonym/blob/Irina_spacy/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3pNVYUXkS0Q",
        "colab_type": "code",
        "outputId": "5e9dec22-32c6-4f9d-8840-f328ee640010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "!python -m venv .env\n",
        "!source .env/bin/activate\n",
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt-get install python3-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: ['/content/.env/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']\n",
            "\n",
            "/bin/bash: .env/bin/activate: No such file or directory\n",
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.0.2)\n",
            "Installing collected packages: preshed, blis, thinc, catalogue, spacy\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.9\n",
            "    Uninstalling spacy-2.1.9:\n",
            "      Successfully uninstalled spacy-2.1.9\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62Yht5zNlslr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgA4knXMlIaa",
        "colab_type": "code",
        "outputId": "6e05b087-92a5-4a95-983c-109b66702230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!sudo apt-get install build-essential python-dev git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5jcdAzglQF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m venv .env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsu5i-7zlwoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!source .env/bin/activate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFEqOER_l2Hr",
        "colab_type": "code",
        "outputId": "086a7342-3a67-4245-94c8-4f54a770f609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.3.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPub0UpdmOxd",
        "colab_type": "code",
        "outputId": "1e6a33f2-c00b-4b67-8e95-0db9a6dae9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python -c \"import os; import spacy; print(os.path.dirname(spacy.__file__))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spacy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMKLXbxgobog",
        "colab_type": "code",
        "outputId": "903d3d2c-9a93-4c49-c6fb-0b979555249c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 638kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (42.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.17.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (8.0.2)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=e5ec82b059ea422a21aa67012d2f47d1a307284e772f71bc6958cb1efda37f60\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-inlofss6/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9xqIXgkoqAH",
        "colab_type": "code",
        "outputId": "3d6d80c8-cec9-46db-8484-fdbcaf1e04ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!python -m spacy download xx_ent_wiki_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xx_ent_wiki_sm==2.2.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-2.2.0/xx_ent_wiki_sm-2.2.0.tar.gz (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 618kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from xx_ent_wiki_sm==2.2.0) (2.2.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (7.3.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (42.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (2.21.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.17.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (2019.11.28)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->xx_ent_wiki_sm==2.2.0) (8.0.2)\n",
            "Building wheels for collected packages: xx-ent-wiki-sm\n",
            "  Building wheel for xx-ent-wiki-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xx-ent-wiki-sm: filename=xx_ent_wiki_sm-2.2.0-cp36-none-any.whl size=3732138 sha256=2e48821fcd16f1345efb2d5b62ae7fd1208d9e395150a5822a909c7b2529b580\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ir03xwzo/wheels/61/ed/d5/02efb1ca98ca2550f28e24fd3931855512448672cfdad7d2c9\n",
            "Successfully built xx-ent-wiki-sm\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-2.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('xx_ent_wiki_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn2ce-c3o73A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "3577d041-7a71-4e7b-92c7-c949dfed148f"
      },
      "source": [
        "!python -m spacy download xx_ent_wiki_sm\n",
        "import xx_ent_wiki_sm\n",
        "nlp = xx_ent_wiki_sm.load()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xx_ent_wiki_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-2.1.0/xx_ent_wiki_sm-2.1.0.tar.gz (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 1.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: xx-ent-wiki-sm\n",
            "  Building wheel for xx-ent-wiki-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xx-ent-wiki-sm: filename=xx_ent_wiki_sm-2.1.0-cp36-none-any.whl size=3744658 sha256=89794c8935b25471060e99eb5a5a740a40737bcc035dcc1607a32cc7cb0c6cd4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1d7oy1ho/wheels/95/72/54/227ed177bc2800291e834b887139e65fb11997d2b7c6ef368a\n",
            "Successfully built xx-ent-wiki-sm\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('xx_ent_wiki_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlsfnobApjMz",
        "colab_type": "code",
        "outputId": "b09e4c3b-5d52-4cce-e3de-8d998534732c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import requests\n",
        "import plac\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "\n",
        "def main():\n",
        "    # For simplicity, we start off with only the blank English Language class\n",
        "    # and no model or pre-defined pipeline loaded.\n",
        "    nlp = English()\n",
        "    rest_countries = RESTCountriesComponent(nlp)  # initialise component\n",
        "    nlp.add_pipe(rest_countries)  # add it to the pipeline\n",
        "    doc = nlp(\"Some text about Colombia and the Czech Republic\")\n",
        "    print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
        "    print(\"Doc has countries\", doc._.has_country)  # Doc contains countries\n",
        "    for token in doc:\n",
        "        if token._.is_country:\n",
        "            print(\n",
        "                token.text,\n",
        "                token._.country_capital,\n",
        "                token._.country_latlng,\n",
        "                token._.country_flag,\n",
        "            )  # country data\n",
        "    print(\"Entities\", [(e.text, e.label_) for e in doc.ents])  # entities\n",
        "\n",
        "\n",
        "class RESTCountriesComponent(object):\n",
        "    \"\"\"spaCy v2.0 pipeline component that requests all countries via\n",
        "    the REST Countries API, merges country names into one token, assigns entity\n",
        "    labels and sets attributes on country tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    name = \"rest_countries\"  # component name, will show up in the pipeline\n",
        "\n",
        "    def __init__(self, nlp, label=\"GPE\"):\n",
        "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
        "        to initialise the matcher with the shared vocab, get the label ID and\n",
        "        generate Doc objects as phrase match patterns.\n",
        "        \"\"\"\n",
        "        # Make request once on initialisation and store the data\n",
        "        r = requests.get(\"https://restcountries.eu/rest/v2/all\")\n",
        "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
        "        countries = r.json()\n",
        "\n",
        "        # Convert API response to dict keyed by country name for easy lookup\n",
        "        # This could also be extended using the alternative and foreign language\n",
        "        # names provided by the API\n",
        "        self.countries = {c[\"name\"]: c for c in countries}\n",
        "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
        "\n",
        "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
        "        patterns = [nlp(c) for c in self.countries.keys()]\n",
        "        self.matcher = PhraseMatcher(nlp.vocab)\n",
        "        self.matcher.add(\"COUNTRIES\", None, *patterns)\n",
        "\n",
        "        # Register attribute on the Token. We'll be overwriting this based on\n",
        "        # the matches, so we're only setting a default value, not a getter.\n",
        "        # If no default value is set, it defaults to None.\n",
        "        Token.set_extension(\"is_country\", default=False)\n",
        "        Token.set_extension(\"country_capital\", default=False)\n",
        "        Token.set_extension(\"country_latlng\", default=False)\n",
        "        Token.set_extension(\"country_flag\", default=False)\n",
        "\n",
        "        # Register attributes on Doc and Span via a getter that checks if one of\n",
        "        # the contained tokens is set to is_country == True.\n",
        "        Doc.set_extension(\"has_country\", getter=self.has_country)\n",
        "        Span.set_extension(\"has_country\", getter=self.has_country)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
        "        are found. Return the Doc, so it can be processed by the next component\n",
        "        in the pipeline, if available.\n",
        "        \"\"\"\n",
        "        matches = self.matcher(doc)\n",
        "        spans = []  # keep the spans for later so we can merge them afterwards\n",
        "        for _, start, end in matches:\n",
        "            # Generate Span representing the entity & set label\n",
        "            entity = Span(doc, start, end, label=self.label)\n",
        "            spans.append(entity)\n",
        "            # Set custom attribute on each token of the entity\n",
        "            # Can be extended with other data returned by the API, like\n",
        "            # currencies, country code, flag, calling code etc.\n",
        "            for token in entity:\n",
        "                token._.set(\"is_country\", True)\n",
        "                token._.set(\"country_capital\", self.countries[entity.text][\"capital\"])\n",
        "                token._.set(\"country_latlng\", self.countries[entity.text][\"latlng\"])\n",
        "                token._.set(\"country_flag\", self.countries[entity.text][\"flag\"])\n",
        "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
        "            doc.ents = list(doc.ents) + [entity]\n",
        "        for span in spans:\n",
        "            # Iterate over all spans and merge them into one token. This is done\n",
        "            # after setting the entities – otherwise, it would cause mismatched\n",
        "            # indices!\n",
        "            span.merge()\n",
        "        return doc  # don't forget to return the Doc!\n",
        "\n",
        "    def has_country(self, tokens):\n",
        "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
        "        is a country. Since the getter is only called when we access the\n",
        "        attribute, we can refer to the Token's 'is_country' attribute here,\n",
        "        which is already set in the processing step.\"\"\"\n",
        "        return any([t._.get(\"is_country\") for t in tokens])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)\n",
        "\n",
        "    # Expected output:\n",
        "    # Pipeline ['rest_countries']\n",
        "    # Doc has countries True\n",
        "    # Colombia Bogotá [4.0, -72.0] https://restcountries.eu/data/col.svg\n",
        "    # Czech Republic Prague [49.75, 15.5] https://restcountries.eu/data/cze.svg\n",
        "    # Entities [('Colombia', 'GPE'), ('Czech Republic', 'GPE')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-10ef2a89-cdac-4e54-8932-20c1b1c3a048.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAk2iNjAHP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECrK5rAO_wKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "b6653b83-fec8-472d-9af0-daa25c1d9f63"
      },
      "source": [
        "#from __future__ import unicode_literals, print_function\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import thinc.extra.datasets\n",
        "\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        "    init_tok2vec=(\"Pretrained tok2vec weights\", \"option\", \"t2v\", Path),\n",
        ")\n",
        "def main(model=None, output_dir=None, n_iter=20, n_texts=2000, init_tok2vec=None):\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # add the text classifier to the pipeline if it doesn't exist\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    # add label to text classifier\n",
        "    textcat.add_label(\"POSITIVE\")\n",
        "    textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "    # load the IMDB dataset\n",
        "    print(\"Loading IMDB data...\")\n",
        "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data()\n",
        "    train_texts = train_texts[:n_texts]\n",
        "    train_cats = train_cats[:n_texts]\n",
        "    print(\n",
        "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
        "            n_texts, len(train_texts), len(dev_texts)\n",
        "        )\n",
        "    )\n",
        "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
        "        optimizer = nlp.begin_training()\n",
        "        if init_tok2vec is not None:\n",
        "            with init_tok2vec.open(\"rb\") as file_:\n",
        "                textcat.model.tok2vec.from_bytes(file_.read())\n",
        "        print(\"Training the model...\")\n",
        "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
        "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
        "        for i in range(n_iter):\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            random.shuffle(train_data)\n",
        "            batches = minibatch(train_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                # evaluate on the dev data split off in load_data()\n",
        "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
        "            print(\n",
        "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
        "                    losses[\"textcat\"],\n",
        "                    scores[\"textcat_p\"],\n",
        "                    scores[\"textcat_r\"],\n",
        "                    scores[\"textcat_f\"],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # test the trained model\n",
        "    test_text = \"This movie sucked\"\n",
        "    doc = nlp(test_text)\n",
        "    print(test_text, doc.cats)\n",
        "\n",
        "    if output_dir is not None:\n",
        "        with nlp.use_params(optimizer.averages):\n",
        "            nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        doc2 = nlp2(test_text)\n",
        "        print(test_text, doc2.cats)\n",
        "\n",
        "\n",
        "def load_data(limit=0, split=0.8):\n",
        "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
        "    # Partition off part of the train data for evaluation\n",
        "    train_data, _ = thinc.extra.datasets.imdb()\n",
        "    random.shuffle(train_data)\n",
        "    train_data = train_data[-limit:]\n",
        "    texts, labels = zip(*train_data)\n",
        "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
        "    split = int(len(train_data) * split)\n",
        "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
        "\n",
        "\n",
        "def evaluate(tokenizer, textcat, texts, cats):\n",
        "    docs = (tokenizer(text) for text in texts)\n",
        "    tp = 0.0  # True positives\n",
        "    fp = 1e-8  # False positives\n",
        "    fn = 1e-8  # False negatives\n",
        "    tn = 0.0  # True negatives\n",
        "    for i, doc in enumerate(textcat.pipe(docs)):\n",
        "        gold = cats[i]\n",
        "        for label, score in doc.cats.items():\n",
        "            if label not in gold:\n",
        "                continue\n",
        "            if label == \"NEGATIVE\":\n",
        "                continue\n",
        "            if score >= 0.5 and gold[label] >= 0.5:\n",
        "                tp += 1.0\n",
        "            elif score >= 0.5 and gold[label] < 0.5:\n",
        "                fp += 1.0\n",
        "            elif score < 0.5 and gold[label] < 0.5:\n",
        "                tn += 1\n",
        "            elif score < 0.5 and gold[label] >= 0.5:\n",
        "                fn += 1\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    if (precision + recall) == 0:\n",
        "        f_score = 0.0\n",
        "    else:\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plac.call(main)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-m None] [-o None] [-n 20] [-t 2000]\n",
            "                             [-t2v None]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-560eac10-2b1d-40ad-a823-6179828adac6.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMosPyUfAJOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import requests\n",
        "import plac\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Doc, Span, Token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd9ncnd9APk_",
        "colab_type": "code",
        "outputId": "2e7244bf-e979-4bf4-d2f2-4a95c799bb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        "    init_tok2vec=(\"Pretrained tok2vec weights\", \"option\", \"t2v\", Path),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-df9592b8e1fe>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnpqT1bh0SIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}